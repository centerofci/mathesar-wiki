# Avi Bryant meeting

**Date**: 2021-01-27  
**Attendees**: Kriti, Brent, Avi

## Questions for Avi
1. Do you think it is a good idea to build an open source version of Dabble DB?
    - Not a terrible idea
    - Build on top of Datasette
    - Thing that was great about Dabble:
        - Data manipulation and exploration built in
            - Data is right there, you can have quick feedback loop
        - Much richer data types
            - Currency, dates, durations
        - Deeply integrated visualization
    - Two different use cases:
        - Have a dataset already, manipulate it.
            - Dabble was stronger at the former, Datasette
        - Data entry and storage from scratch
            - Spreadsheets and google forms makes this easy
            - CRM is a usecase for more complex data
        - People did use Dabble for bespoke applications, but workflow was awkward
        - If starting again, would focus on readonly datasets first.
            - Systems focuses either on editing or on reporting
                - e.g. JotSpot contemporary to Dabble DB
                - Notion modern equivalent
                - Cannot deal well with large sizes of data
            -  spreadsheet and BI systems/analytics are reporting heavy but do not focus as much on content entry 
            -  Dabble was a reporting heavy tool
        -  Editing primary tool - always about the content
        -  Reporting primary tool - configuration heavy (create a report)
        -  Sweet spot: direct manipulation like content creation tools, geared towards high cardinality data like reporting tools tend to be
        -  need to have rich ways of manipulating both individual rows as well as aggregation tools
            -  Grouping was primary in Dabble, each group is primarily is a row
                -  GROUPBY as a primary unit
        -  GROUPBY and rich data types go together and have a lot of synergy
            -  You're not grouping by existing fields, you're grouping by transformation
                -  e.g. date, extract week/quarter/year to group on it
                -  address, group by state, group, continent
                -  add a new column that extracts this info, group by it
                -  how much work can the system do to group by automatically
                    -  then figure out fields to summarize by
                    -  latency, get the P95 out
                    -  exposing these really easily
        -  go from here's the raw data to extract group by to group by that, then find what to summarize by and/or sort by
        -  I can see that you've grouped by date and done a sum, now you probably want to see a line graph
            -  or if you have a few scalars, then do a scatterplot
        -  group by state, let me show you a map
        -  richer data types let you suggest a visualization appropriate to what you want
    -  you can imagine all this functionality as a wizard or a dialog box with lots of configuration/options, but Dabble showed this really intuitively in the flow of things
    -  those are the elements we're playing with
    -  accessibility, ease of deployment, OSS of Datasette, but had Dabble's grouping and visualizing and summarizing
1. What pitfalls do you think we'd run into?
    - expressivity of a SQL query or python code, there's a lot you can do and you're always going to run up against things that it can't do
        - we spent a lot of time worrying about that but we shouldn't have
        - 80/20 - leave 20% of people out in the cold, but focus on getting the 80% case well
        - people who are working on really niche things will go write SQL
        - totally fine to say "this is a single table tool, you need to import denormalized data", or one level deeper
    - Dabble was very committed to the idea of not writing code, only point and click
    - Scalability of a database system that's accepting writes
        - logging transactions, replicating
        - Focused on read only DB at first, UI will be a lot simpler but so will infrastructure
        - Dabble ended up building their own database system, don't do that
    - Load all data into the browser and we'll do it all in JavaScript
        - you WILL run into scaling issues, you're setting yourself up for failure
1. Who were the primary users of Dabble DB? What can you tell us about them?
    - A lot of custom project/customer/bookkeeping internal business systems
        - e.g. Orchestra managing shows, musicians, tickets
            - "how do ticket sales for classical vs. popular music compare?"
        - e.g. issue tracking with custom workflows and data
        - bookkeeping with a lot of metadata was very common
            - analytics on cashflow/revenue with context of domain specific structures
    - Splunk UI is a good example
        - specific users: devops/developers
        - what would be the equivalent of Splunk where you were focusing on a "sale" or "transaction" as the primary unit? (stores, school programs, Shopify etc,)
            - common data: time, amount, location, other stuff is more custom
            - organize your system around that, you could offer a lot for free
            - that's fairly business oriented and may not mesh with the focus
                - Dabble's paying customers were mostly businesses and they care about sales
    - you could do a small number of integrations to get feed of core data (Stripe, etc.) without much work
        - Dabble couldn't do that at the time
        - they did play with Google Analytics data, but that's not as useful
        - Google Analytics was a good demo, one click auth, and they had a lot of data without work
    - Central narrow use case to start with and demo
    - Blank system makes people bounce
    - Populating with data they care about engaged people right away
        - Didn't have a lot of APIs back then, flow was about spreadsheet pasting
        - Nowadays: Stripe, Twilio, Amazon, lots of business APIs
1. Would you be able to share how many people used Dabble DB?
    - 1000s of paying customers
    - 10k-100ks of free trial or free version customers
1. Did you have trouble finding a market or explaining what is was to people?
    - Used to reference FileMaker or Access and people knew what we were talking about
    - Every business runs on Access/FileMaker that consultants threw together
    - People use spreadsheets
    - Related pitch: Spreadsheets are used for monte carlo simulation forecasts and it's very difficult to adjust forecasts, and there's no way to feed real numbers back into previous forecast and integrate into new predications
    - Making Bayes DB more accessible for business users will be a bigger impact on the world, more difficult
        - No one in that space cares about industry users and non statisticians 
1. Are you willing to share why you sold/shut down Dabble DB?
    - Took VC money geared towards scaling, felt an obligation to get investors return
    - Broke even for small team (5-6), without VC money, we would have kept growing it
    - Airtable started soon after we shut down
    - Not good at business side and not good at scaling/infrastructure side
1. Architecture
    - Should we use Spark or PySpark?
        - Depends on scale, up to a few GB, SQLite is okay
    - Maybe pandas?
        - Pandas might make more sense, in memory, natural for pandas, data frame wrangling options
    - From a deployment PoV, you could do it with R and pandas and do in memory, but you could also have it in a SQLite file and latency would be slightly higher but it would work for many use cases
    - Requiring them to load data into memory increases cost of deployment
    - Scaling question â€“ 1 GB vs 100 GB vs few terabytes
        - Most use cases will fit in memory
    - Do whatever's faster to prototype
    - SQL is optimized for joins, data frames do not have that
    - Dabble - went to a lot of effort to make joins comprehensible to people not using SQL
        - Not sure where it came out.
        - From a reporting PoV, try to get a really denormalized table, one wide view
        - e.g. at Stripe, get data from a lot of columns from other tables (e.g. merchant, etc.), but all related to transactions
        - incrementally build up denormalized table
        - writing a SQL query with a lot of joins is a huge pain for people not used to it

### Didn't get to these questions
1. Are you aware of any DabbleDB-like products on the market currently?
1. Are there any other lessons you learned from Dabble DB that could be useful to us?
1. Is there anyone else we could talk to about Dabble DB?
1. Would you like to continue to be kept in the loop about what we do?
1. What do do you think we need to get right to be successful?
1. Dabble DB codebase
1. Ideas to grow?
1. Anything else to add?

## Thoughts during call
- Datasette
- Incremental development of data model and queries (like a spreadsheet)
- can we GUI "group by" or "withColumnAs"
- This should be PySpark under the hood!
- If we want transactional, beware of what the underlying language allows.
- Are there data sources that some sect of non-profits are generally interested in? Do they have APIs?
- Jot spot