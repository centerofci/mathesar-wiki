---
title: SJ Klein
description: 
published: true
date: 2021-04-19T21:26:50.865Z
tags: 
editor: markdown
dateCreated: 2021-04-19T20:22:32.684Z
---

# SJ Klein notes

**Date**: 2021-02-17  
**Attendees**: Kriti, Brent, SJ

## Meeting notes

- Introductions
- SJ has been thinking about decentralization as a prerequisite to sane governance and distribution of power
    - Not as a category in itself (decentralized vs. non-decentralized)
    - Lots of things need a decentralizing layer
        - Doesn't need to advertise as decentralized, e.g. DNS
    - No one thought about DNS as "the decentralized web"
    - Mail and messaging are not very well decentralized, but just "work"
- SJ's work
    - Rebuilding social interconnection between people and projects without intermediaries
    - Decentralization is generally good
    - You don't need to replace something, just get tens of thousands of people excited about it
- OpenStreetMaps is interesting
    - Inspired by Wikipedia
    - Weren't confident that they could satisfy their needs using individual editors, so built on industry partnerships like Mapbox
    - Current OSM events are dominated by industry players
    - Mapbox is now withdrawing from participation, and they were a big contributor
    - OSM and Wikipedia could have shared resposibilities for infrastructure, but didn't.
    - Don't want to make the same mistake now.
- Data is spread across different repositories, some are public, some are private
    - Can be used to train proprietary ML models totally separate from the data that went in
    - Systemic flaws in reasoning just like closed source code
    - Process flaws that allow people to insert exploits
- Thinking of ML models as a central public good
    - We'll have an upcoming decade of ML models becoming the best way to understand something
        - Could turn into something like search, where paid placement dominates results
        - With an ML model, there's no way to know
- We need better examples in this area, stuff to point to while figuring out law and policy
    - People will pay for someone to train a model, so it's sustainable
    - "Here's a transparent network built on open data"
    - People want someone to run the equivalent of OpenRefine on data for clean and enriched data
- Choose training methods with more hooks
    - Current work: People are able to come up with more open models that match the performance of the inscrutable ones
    - Training datasets and process being public means that other people can do the analysis
- http://sidn.csail.mit.edu/
- Underlay & KFG work
    - Schema editor: https://underlay.github.io/playground/schema-editor/index.html
    - Another schema editor, sans algebra: https://underlay.github.io/playground/toml-schemas/
    - PubPub generates structured data e.g. review pipeline, versions of a document, tags
        - these steps can be parallelized
        - Everything you make carries an identifier with it
    - Underlay IDs are immutable, PubPub gives everything a DOI
- https://notes.knowledgefutures.org/pub/belji1gd/release/2
- Link these three:
    - 1. Defining workflows
    - 2. Datasette or openrefine, hosted play 
    - 3. Visualizations
        - 1. datasette.io
        - 2. https://etl.linkedpipes.com/
        - 3. visualization options (OWID)
- Collaborative public datasets - nothing to do that now, lots of use cases
    - Almost no overlap with Airtable
    - Will generate "collections" in Underlay
    - A little bit of work to connect to Wikidata
- A public namespace where anyone can define one of these things is missing
    - Link to full datasets
    - Colab and Jupyter Notebooks have unresolved feature requests about this, have a dataset and I want a bunch of notebooks to share it
- Jupyter team is now a small nonprofit, and they have a pretty stable roadmap for people who just want Jupyter implementation
    - They'd be good to talk to
- SJ could introduce us to Jupyter lead devs and Our World in Data

## Previous Slack notes

### Kriti and SJ

- Links
	https://unfoldresearch.com/
- A {Structured Learning} tool that lets people create Papers With Data or Textbooks With Code as part of an open global texture.
	- Papers with data would be pretty well defined -- every paper should eventually have a summary, linked protocols, methods, claims, and data (if not visuals), scite-like summaries of their support in other literature, and membership in clusters of related work.
	- There could be a way to define the scope of an overview paper or metastudy, and autogenerate that (with tweakable parameters), while also making room for hand-curated overviews.
	- Most searches on a topic really want such an overview if it exists ... today the best available overview only has ~60% coverage of the field, is at least a year old, and doesn't say anything (pro or con) about more recent bold claims / fads / hoaxes (which tend to get the most visibility and attention in the popular press).   Overviews also are not explicit about how they select the research it included in the summary.
		- [This is readily gamed in many fields like Ag and Pharma and Nutrition where the benefits of faking cconsensus far outweigh the risks.]
	- This is related to a few of the ideas you mentioned; and could be the mechanism for generating the knowledge graphs you had in mind.  (A good text or instruction often needs to draw on the best available research and observations; and the community of 10,000+ grad students + teachers building course-packs and reading lists for their classes, where that is the closest thing to a text, are all in nee of such a tool)
- not for papers, for knowledge.  starting with the workflow of
	- [1] "researching something to understand it" which is a common workflow for students, everyday queries, profesional researchers, and machines alike
		- with [i] a way of finding + browsing papers and textbooks that gives you permalinks to views that you can share with others
		- [ii] a way of annotating + attaching new data / documents to what you're reading, in a way visible to others
		- [iii] a way of summarizing what you're [viewing], including extracted data, infoboxes, concepts, or a visualization of a query from that single source
			- Wikipedia specializes in infoboxes and navboxes.
			- Our World in Data specializes in labelled numerical 2D charts w/ a mix of highlighted and hidden elements.
			- Google Public Data Explorer specializes in 4-dim charts: 2 axes + 2 dimensional points (size + color).
		- [iv] a way of generating a summary across [documents/sources], such as an overview or meta-study, or a visualization of a query that draws from many sources
			- Generating catalogs of important public tools along the way :
				- [A] a library of query interfaces, that help you write executable queries w/ reasonable hints + autocompliete
				- [B] a library of tools for generating common overviews: text summaries, infoboxes, sparklines, graphs, multi-dim charts
- Other interesting projects, relying on similar components: (edited) 
	- [2] "I Can't Believe It's Not Science/Nature!", a global overlay journal that autogenerates the best available open summary of any paper on request, building on the above, especially I.ii + I.iii
	- [3] Papers with Data, a complement to Papers w/ Code -- a collection of papers with extracted data + summaries, aligned around shared schemas, protocols, or hypotheses
	- [4] Textbooks with {Data, Code}, a generalization of SICP/SICM  ("Structure & Intepretation" series), accessible to anyone compiling a text
- 1 feels like better search; 2 like the best journal experience; 3 like syncretic exploration of a research tpic, 4 like syncretic development of courses.
- MVP
	- I have been thinking about the idea of "magical document annotation" where this is an open tool you use to take notes (and perceive those that exist), building on the many layers of annotation that are out there but not accessible or usable in this way
		- Something I speced out at one point, which I have yet to see in the wild: "a science annotator": https://notes.knowledgefutures.org/pub/annotator/release/2
- anot8
	- The 'searchable + curatable index' piece is what seems missing - there's definitely no persistent place that someone could (say) clean up + annotate a few thousand densely interlinked summaries of COVID-19 research papers and have those summaries found by most others studying those papers.
	- This might involve for components like
		- a frbr namespace for clustering similar documents, whose aggregate annotations are of interest to the same audiences
		- a service namespace for clustering related services, that get you the transformation you need to resolve a query
		- a folk agency of agents that find the closest  approximation to what you're looking for

## Brent and SJ

Brent:  I looked through https://notes.knowledgefutures.org/pub/annotator/release/2 , and it looks similar in many ways to what we've considered.  I have a couple questions:
1. Have you considered having notes as documents?  To me, this enables elegant conversation threads, and also provides a unified way to consider comments / notes / referee reports that refer to a document and other documents that refer to a document (e.g., citing papers).
1. The versioning and hashing feels very Merkle-Tree-ish to me.  Is this what you were thinking three?  It's a bit vague in the doc.

SJ:
- Notes are certainly a type of document, but there's a bimodal distribution in current usage with most being multi-page documents (papers, essays, webpages)  or sub-paragraph notes (tweets, citations, comments).
- You don't need a ledger or merkle trees for this simple stage, just a content-identifier.  If you have a centralized way to track documents you don't need hashes at all (save that they're a convenient source of unique IDs). 
    - You'll need explicit equivalence classes between different versions/format changes, so you can't rely on the hash sufficing to find matches (say, across comments made by others on variations of the same doc)
